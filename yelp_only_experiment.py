#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Yelp-Only å¯¹ç…§å®éªŒ
=================

å®éªŒç›®æ ‡ï¼š
1. å®Œå…¨åŸºäº Yelp Open Dataset è®­ç»ƒå’Œè¯„ä¼° BayesReviewNet
2. æ¶ˆé™¤è·¨åŸŸå¹²æ‰°ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å•åŸŸå†…çš„æ€§èƒ½ä¸Šé™
3. ä¸º Amazonâ†’Yelp è·¨åŸŸè¿ç§»æä¾›å¯¹ç…§åŸºçº¿

æ•°æ®åˆ’åˆ†ï¼š
- Training Set (70%): ç”¨äºå­¦ä¹ å…ˆéªŒæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡(CPD)
- Validation Set (15%): ç”¨äºè¶…å‚æ•°è°ƒä¼˜å’Œé˜ˆå€¼é€‰æ‹©
- Test Set (15%): ä¸¥æ ¼çš„æœ€ç»ˆè¯„ä¼°

å¼±æ ‡ç­¾æ„é€ ï¼š
- åŸºäºå¹³å°æ ‡æ³¨ï¼ˆfiltered reviewsï¼‰
- è¡Œä¸ºå¼‚å¸¸ï¼ˆburstiness, rating varianceï¼‰
- æ–‡æœ¬å¼‚å¸¸ï¼ˆé•¿åº¦ã€é‡å¤åº¦ã€æƒ…ç»ªï¼‰
- ç½‘ç»œå¼‚å¸¸ï¼ˆreviewer-business å›¾ç»“æ„ï¼‰
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import pandas as pd
import numpy as np
from typing import Tuple, Dict
import json

from src.utils.logging import setup_logger
from src.utils.config import load_config, ensure_dir
from src.utils.io import save_data
from src.preprocessing import YelpPreprocessor
from src.preprocessing.weak_labeling import construct_weak_label
from src.features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
from src.bayes import BayesianNetworkStructure, CPDLearner, BayesianInference
from src.evaluation import evaluate_model
from src.evaluation.metrics import find_optimal_threshold

logger = setup_logger("yelp_only_exp")


class YelpOnlyExperiment:
    """
    Yelp-Only å¯¹ç…§å®éªŒ
    
    å®Œå…¨åŸºäº Yelp æ•°æ®è®­ç»ƒå’Œè¯„ä¼°ï¼Œä¸ä½¿ç”¨ä»»ä½• Amazon æ•°æ®
    """
    
    def __init__(self, config_path: str = 'configs/default.yaml', use_sampling: bool = True):
        """
        åˆå§‹åŒ–å®éªŒ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            use_sampling: æ˜¯å¦ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‡‡æ ·è®¾ç½®
        """
        self.config = load_config(config_path)
        self.use_sampling = use_sampling
        self.results = {}
        
        logger.info("="*80)
        logger.info("Yelp-Only å¯¹ç…§å®éªŒ")
        logger.info("="*80)
        logger.info("å®éªŒç›®æ ‡: è¯„ä¼° BayesReviewNet åœ¨å•åŸŸå†…çš„æ€§èƒ½ä¸Šé™")
        logger.info("æ•°æ®æ¥æº: Yelp Open Dataset (ä»…)")
        logger.info("="*80)
    
    def run(self, 
            sample_size: int = None,
            train_ratio: float = 0.70,
            val_ratio: float = 0.15,
            test_ratio: float = 0.15,
            random_seed: int = 42) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ Yelp-Only å®éªŒ
        
        Args:
            sample_size: é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºå…¨é‡ï¼‰
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            random_seed: éšæœºç§å­
            
        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        np.random.seed(random_seed)
        
        # ========== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ==========
        logger.info("\nã€æ­¥éª¤1ã€‘åŠ è½½ Yelp æ•°æ®")
        yelp_df = self._load_yelp_data(sample_size)
        
        # ========== æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹ ==========
        logger.info("\nã€æ­¥éª¤2ã€‘ç‰¹å¾å·¥ç¨‹")
        yelp_df = self._extract_features(yelp_df)
        
        # ========== æ­¥éª¤3: æ„é€ å¼±æ ‡ç­¾ ==========
        logger.info("\nã€æ­¥éª¤3ã€‘æ„é€ å¼±ç›‘ç£æ ‡ç­¾")
        yelp_df = self._construct_weak_labels(yelp_df)
        
        # ========== æ­¥éª¤4: æ•°æ®é›†åˆ’åˆ† ==========
        logger.info("\nã€æ­¥éª¤4ã€‘æ•°æ®é›†åˆ’åˆ† (Train/Val/Test)")
        train_df, val_df, test_df = self._split_dataset(
            yelp_df, train_ratio, val_ratio, test_ratio, random_seed
        )
        
        # ========== æ­¥éª¤5: è®­ç»ƒè´å¶æ–¯ç½‘ç»œ ==========
        logger.info("\nã€æ­¥éª¤5ã€‘è®­ç»ƒè´å¶æ–¯ç½‘ç»œ (ä»…åŸºäº Yelp Training Set)")
        structure, cpd_learner = self._train_bayesian_network(train_df)
        
        # ========== æ­¥éª¤6: éªŒè¯é›†è°ƒä¼˜ ==========
        logger.info("\nã€æ­¥éª¤6ã€‘éªŒè¯é›†æ€§èƒ½è¯„ä¼°ä¸é˜ˆå€¼é€‰æ‹©")
        val_results, optimal_threshold = self._evaluate_on_validation(
            val_df, structure, cpd_learner
        )
        
        # ========== æ­¥éª¤7: æµ‹è¯•é›†è¯„ä¼° ==========
        logger.info("\nã€æ­¥éª¤7ã€‘æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°")
        test_results = self._evaluate_on_test(
            test_df, structure, cpd_learner, optimal_threshold
        )
        
        # ========== æ­¥éª¤8: ä¿å­˜ç»“æœ ==========
        logger.info("\nã€æ­¥éª¤8ã€‘ä¿å­˜å®éªŒç»“æœ")
        self._save_results(train_df, val_df, test_df, val_results, test_results)
        
        # ========== æ­¥éª¤9: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š ==========
        logger.info("\nã€æ­¥éª¤9ã€‘ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š")
        self._generate_comparison_report(val_results, test_results)
        
        return {
            'validation': val_results,
            'test': test_results,
            'optimal_threshold': optimal_threshold,
            'data_split': {
                'train': len(train_df),
                'val': len(val_df),
                'test': len(test_df)
            }
        }
    
    def _load_yelp_data(self, sample_size: int = None) -> pd.DataFrame:
        """
        åŠ è½½ Yelp åŸå§‹æ•°æ®
        
        Args:
            sample_size: é‡‡æ ·å¤§å°ï¼ˆä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰
            
        Returns:
            æ ‡å‡†åŒ–çš„ DataFrame
        """
        yelp_preprocessor = YelpPreprocessor(
            self.config['data_paths']['yelp']['raw_dir']
        )
        
        # ç¡®å®šæœ€ç»ˆé‡‡æ ·å¤§å°
        if sample_size is not None:
            # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆ
            final_sample_size = sample_size
            logger.info(f"  ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„é‡‡æ ·å¤§å°: {final_sample_size}")
        elif self.use_sampling and self.config.get('sampling', {}).get('enabled', False):
            # ä½¿ç”¨é…ç½®æ–‡ä»¶
            final_sample_size = self.config['sampling'].get('yelp_sample_size', None)
            logger.info(f"  ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é‡‡æ ·å¤§å°: {final_sample_size}")
        else:
            # å…¨é‡æ•°æ®
            final_sample_size = None
            logger.info(f"  ä½¿ç”¨å…¨é‡æ•°æ®ï¼ˆæ— é‡‡æ ·ï¼‰")
        
        df = yelp_preprocessor.load_and_standardize(final_sample_size)
        
        logger.info(f"  âœ“ åŠ è½½ Yelp æ•°æ®: {len(df)} æ¡è®°å½•")
        logger.info(f"  - æ—¶é—´èŒƒå›´: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
        logger.info(f"  - ç”¨æˆ·æ•°: {df['user_id'].nunique()}")
        logger.info(f"  - å•†å®¶æ•°: {df['item_id'].nunique()}")
        
        return df
    
    def _extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æå–å¤šè§†è§’ç‰¹å¾
        
        Args:
            df: åŸå§‹ DataFrame
            
        Returns:
            æ·»åŠ ç‰¹å¾åçš„ DataFrame
        """
        # æ–‡æœ¬ç‰¹å¾
        logger.info("  â†’ æå–æ–‡æœ¬ç‰¹å¾...")
        text_extractor = TextFeatureExtractor()
        df = text_extractor.extract(df)
        
        # è¡Œä¸ºç‰¹å¾
        logger.info("  â†’ æå–è¡Œä¸ºç‰¹å¾...")
        behavior_extractor = BehaviorFeatureExtractor()
        df = behavior_extractor.extract(df)
        
        # ç‰¹å¾ç¦»æ•£åŒ–
        logger.info("  â†’ ç‰¹å¾ç¦»æ•£åŒ–...")
        discretizer = FeatureDiscretizer()
        df = discretizer.discretize(df)
        
        # ç‰¹å¾è´¨é‡æ£€æŸ¥
        self._check_feature_quality(df)
        
        return df
    
    def _check_feature_quality(self, df: pd.DataFrame):
        """
        æ£€æŸ¥ç‰¹å¾è´¨é‡ï¼Œå‘å‡ºè­¦å‘Š
        
        Args:
            df: DataFrame
        """
        discrete_cols = [c for c in df.columns if c.endswith('_discrete')]
        
        logger.info("  â†’ ç‰¹å¾è´¨é‡æ£€æŸ¥:")
        
        warnings = []
        
        for col in discrete_cols:
            # æ£€æŸ¥å”¯ä¸€å€¼æ•°é‡
            n_unique = df[col].nunique()
            missing_rate = df[col].isna().mean()
            
            if n_unique == 1:
                warnings.append(f"    âš  {col}: æ— åŒºåˆ†åº¦ï¼ˆä»…1ä¸ªå”¯ä¸€å€¼ï¼‰")
            elif missing_rate > 0.5:
                warnings.append(f"    âš  {col}: é«˜ç¼ºå¤±ç‡ ({missing_rate*100:.1f}%)")
        
        if warnings:
            logger.warning("  å‘ç°ç‰¹å¾è´¨é‡é—®é¢˜:")
            for w in warnings:
                logger.warning(w)
        else:
            logger.info("    âœ“ æ‰€æœ‰ç‰¹å¾è´¨é‡æ­£å¸¸")
    
    def _construct_weak_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„é€  Yelp å¼±ç›‘ç£æ ‡ç­¾
        
        åŸºäºå¤šä¸ªä¿¡å·ï¼š
        1. å¹³å°æ ‡æ³¨ (filtered reviews)
        2. è¡Œä¸ºå¼‚å¸¸
        3. æ–‡æœ¬å¼‚å¸¸
        4. ç½‘ç»œå¼‚å¸¸
        
        Args:
            df: DataFrame
            
        Returns:
            æ·»åŠ  weak_label çš„ DataFrame
        """
        df = construct_weak_label(df, 'yelp')
        
        # ç»Ÿè®¡å¼±æ ‡ç­¾åˆ†å¸ƒ
        label_dist = df['weak_label'].value_counts()
        total = len(df)
        
        logger.info(f"  âœ“ å¼±æ ‡ç­¾æ„é€ å®Œæˆ:")
        logger.info(f"    - Fraud (1): {label_dist.get(1, 0)} ({label_dist.get(1, 0)/total*100:.1f}%)")
        logger.info(f"    - Normal (0): {label_dist.get(0, 0)} ({label_dist.get(0, 0)/total*100:.1f}%)")
        logger.info(f"    - Missing: {df['weak_label'].isna().sum()}")
        
        return df
    
    def _split_dataset(self,
                       df: pd.DataFrame,
                       train_ratio: float,
                       val_ratio: float,
                       test_ratio: float,
                       random_seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        åˆ’åˆ†æ•°æ®é›†ä¸º Train / Validation / Test
        
        ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿æ ‡ç­¾åˆ†å¸ƒä¸€è‡´
        
        Args:
            df: å®Œæ•´ DataFrame
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            random_seed: éšæœºç§å­
            
        Returns:
            (train_df, val_df, test_df)
        """
        from sklearn.model_selection import train_test_split
        
        # éªŒè¯æ¯”ä¾‹
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
        
        # ç§»é™¤ weak_label ç¼ºå¤±çš„æ ·æœ¬
        df_valid = df[df['weak_label'].notna()].copy()
        logger.info(f"  æœ‰æ•ˆæ ·æœ¬ï¼ˆweak_labeléç©ºï¼‰: {len(df_valid)} / {len(df)}")
        
        # ç¬¬ä¸€æ¬¡åˆ’åˆ†: train vs (val+test)
        train_df, temp_df = train_test_split(
            df_valid,
            train_size=train_ratio,
            stratify=df_valid['weak_label'],
            random_state=random_seed
        )
        
        # ç¬¬äºŒæ¬¡åˆ’åˆ†: val vs test
        val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)
        val_df, test_df = train_test_split(
            temp_df,
            train_size=val_ratio_adjusted,
            stratify=temp_df['weak_label'],
            random_state=random_seed
        )
        
        logger.info(f"  âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        logger.info(f"    - Training:   {len(train_df)} ({len(train_df)/len(df_valid)*100:.1f}%)")
        logger.info(f"    - Validation: {len(val_df)} ({len(val_df)/len(df_valid)*100:.1f}%)")
        logger.info(f"    - Test:       {len(test_df)} ({len(test_df)/len(df_valid)*100:.1f}%)")
        
        # éªŒè¯æ ‡ç­¾åˆ†å¸ƒ
        logger.info(f"\n  æ ‡ç­¾åˆ†å¸ƒä¸€è‡´æ€§æ£€æŸ¥:")
        for name, subset in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:
            fraud_rate = subset['weak_label'].mean()
            logger.info(f"    {name}: Fraudç‡ = {fraud_rate*100:.1f}%")
        
        return train_df, val_df, test_df
    
    def _train_bayesian_network(self,
                                 train_df: pd.DataFrame) -> Tuple[BayesianNetworkStructure, CPDLearner]:
        """
        è®­ç»ƒè´å¶æ–¯ç½‘ç»œ
        
        å®Œå…¨åŸºäº Yelp Training Setï¼Œä¸ä½¿ç”¨ä»»ä½•å¤–éƒ¨æ•°æ®
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            
        Returns:
            (structure, cpd_learner)
        """
        # å®šä¹‰ç½‘ç»œç»“æ„
        structure = BayesianNetworkStructure()
        structure.define_structure('default')
        
        logger.info(f"  âœ“ DAG ç»“æ„: {len(structure.edges)} æ¡è¾¹")
        logger.info(f"  æ‹“æ‰‘æ’åº: {structure.get_topological_order()}")
        
        # å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
        cpd_learner = CPDLearner(structure)
        cpd_learner.learn_cpds(train_df, smoothing=1.0)
        
        logger.info(f"  âœ“ CPD å­¦ä¹ å®Œæˆ")
        logger.info(f"    - è®­ç»ƒæ ·æœ¬: {len(train_df)}")
        logger.info(f"    - å­¦ä¹ èŠ‚ç‚¹: {len(cpd_learner.cpds)}")
        
        return structure, cpd_learner
    
    def _evaluate_on_validation(self,
                                 val_df: pd.DataFrame,
                                 structure: BayesianNetworkStructure,
                                 cpd_learner: CPDLearner) -> Tuple[Dict, float]:
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶é€‰æ‹©æœ€ä¼˜é˜ˆå€¼
        
        Args:
            val_df: éªŒè¯æ•°æ®
            structure: ç½‘ç»œç»“æ„
            cpd_learner: CPDå­¦ä¹ å™¨
            
        Returns:
            (evaluation_results, optimal_threshold)
        """
        # æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        val_df = inference.infer_posterior(val_df.copy(), target_variable='weak_label')
        
        # åéªŒæ¦‚ç‡ç»Ÿè®¡
        posterior = val_df['weak_label_posterior_prob'].dropna()
        logger.info(f"  åéªŒæ¦‚ç‡åˆ†å¸ƒ:")
        logger.info(f"    - å‡å€¼: {posterior.mean():.4f}")
        logger.info(f"    - ä¸­ä½æ•°: {posterior.median():.4f}")
        logger.info(f"    - æ ‡å‡†å·®: {posterior.std():.4f}")
        logger.info(f"    - æœ€å°å€¼: {posterior.min():.4f}")
        logger.info(f"    - æœ€å¤§å€¼: {posterior.max():.4f}")
        
        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        logger.info(f"\n  å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼ (åŸºäº F1-Score)...")
        optimal_result = find_optimal_threshold(val_df, metric='f1')
        optimal_threshold = optimal_result['best_threshold']
        
        logger.info(f"  âœ“ æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¯„ä¼°
        results = evaluate_model(val_df, threshold=optimal_threshold)
        metrics = results['metrics']
        
        logger.info(f"\n  éªŒè¯é›†æ€§èƒ½ (é˜ˆå€¼={optimal_threshold:.4f}):")
        logger.info(f"    - Precision: {metrics['precision']:.4f}")
        logger.info(f"    - Recall:    {metrics['recall']:.4f}")
        logger.info(f"    - F1-Score:  {metrics['f1']:.4f}")
        logger.info(f"    - ROC-AUC:   {metrics.get('roc_auc', 'N/A')}")
        
        return results, optimal_threshold
    
    def _evaluate_on_test(self,
                          test_df: pd.DataFrame,
                          structure: BayesianNetworkStructure,
                          cpd_learner: CPDLearner,
                          threshold: float) -> Dict:
        """
        åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
        
        Args:
            test_df: æµ‹è¯•æ•°æ®
            structure: ç½‘ç»œç»“æ„
            cpd_learner: CPDå­¦ä¹ å™¨
            threshold: åˆ†ç±»é˜ˆå€¼
            
        Returns:
            evaluation_results
        """
        # æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        test_df = inference.infer_posterior(test_df.copy(), target_variable='weak_label')
        
        # åéªŒæ¦‚ç‡ç»Ÿè®¡
        posterior = test_df['weak_label_posterior_prob'].dropna()
        logger.info(f"  åéªŒæ¦‚ç‡åˆ†å¸ƒ:")
        logger.info(f"    - å‡å€¼: {posterior.mean():.4f}")
        logger.info(f"    - ä¸­ä½æ•°: {posterior.median():.4f}")
        logger.info(f"    - æ ‡å‡†å·®: {posterior.std():.4f}")
        
        # è¯„ä¼°
        results = evaluate_model(test_df, threshold=threshold)
        metrics = results['metrics']
        
        logger.info(f"\n  æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½ (é˜ˆå€¼={threshold:.4f}):")
        logger.info(f"    - Precision: {metrics['precision']:.4f}")
        logger.info(f"    - Recall:    {metrics['recall']:.4f}")
        logger.info(f"    - F1-Score:  {metrics['f1']:.4f}")
        logger.info(f"    - ROC-AUC:   {metrics.get('roc_auc', 'N/A')}")
        
        return results
    
    def _save_results(self,
                      train_df: pd.DataFrame,
                      val_df: pd.DataFrame,
                      test_df: pd.DataFrame,
                      val_results: Dict,
                      test_results: Dict):
        """
        ä¿å­˜å®éªŒç»“æœ
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            val_df: éªŒè¯æ•°æ®
            test_df: æµ‹è¯•æ•°æ®
            val_results: éªŒè¯é›†ç»“æœ
            test_results: æµ‹è¯•é›†ç»“æœ
        """
        output_dir = Path('data/experiments/yelp_only')
        ensure_dir(str(output_dir))
        
        # ä¿å­˜æ•°æ®
        save_data(train_df, str(output_dir / 'train.parquet'))
        save_data(val_df, str(output_dir / 'validation.parquet'))
        save_data(test_df, str(output_dir / 'test.parquet'))
        
        # ä¿å­˜ç»“æœ
        results_summary = {
            'experiment': 'Yelp-Only Baseline',
            'data_split': {
                'train': len(train_df),
                'validation': len(val_df),
                'test': len(test_df)
            },
            'validation_metrics': val_results['metrics'],
            'test_metrics': test_results['metrics']
        }
        
        with open(output_dir / 'results.json', 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        logger.info(f"  âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/")
    
    def _generate_comparison_report(self, val_results: Dict, test_results: Dict):
        """
        ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
        
        Args:
            val_results: éªŒè¯é›†ç»“æœ
            test_results: æµ‹è¯•é›†ç»“æœ
        """
        logger.info("\n" + "="*80)
        logger.info("Yelp-Only å®éªŒæ€»ç»“")
        logger.info("="*80)
        
        logger.info("\nã€æ¨¡å‹æ€§èƒ½ã€‘")
        logger.info("  éªŒè¯é›†:")
        for metric, value in val_results['metrics'].items():
            logger.info(f"    {metric:12s}: {value:.4f}")
        
        logger.info("\n  æµ‹è¯•é›†:")
        for metric, value in test_results['metrics'].items():
            logger.info(f"    {metric:12s}: {value:.4f}")
        
        logger.info("\nã€å…³é”®è§‚å¯Ÿã€‘")
        
        # æ³›åŒ–èƒ½åŠ›
        val_f1 = val_results['metrics']['f1']
        test_f1 = test_results['metrics']['f1']
        generalization_gap = val_f1 - test_f1
        
        logger.info(f"  æ³›åŒ–å·®è· (Val F1 - Test F1): {generalization_gap:+.4f}")
        if abs(generalization_gap) < 0.02:
            logger.info("    â†’ æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
        elif generalization_gap > 0:
            logger.info("    â†’ å­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆ")
        else:
            logger.info("    â†’ æµ‹è¯•é›†æ€§èƒ½ä¼˜äºéªŒè¯é›†ï¼ˆæ­£å¸¸æ³¢åŠ¨ï¼‰")
        
        logger.info("\nã€å¯¹ç…§å®éªŒæ„ä¹‰ã€‘")
        logger.info("  æ­¤ç»“æœä¸º Amazonâ†’Yelp è·¨åŸŸè¿ç§»æä¾›å¯¹ç…§åŸºçº¿:")
        logger.info("  - å¦‚æœè·¨åŸŸè¿ç§» F1 < Yelp-Only F1:")
        logger.info("    â†’ è¯´æ˜è·¨åŸŸå¹²æ‰°ç¡®å®å­˜åœ¨ï¼Œéœ€è¦åŸŸé€‚åº”")
        logger.info("  - å¦‚æœè·¨åŸŸè¿ç§» F1 â‰ˆ Yelp-Only F1:")
        logger.info("    â†’ è¯´æ˜è·¨åŸŸè¿ç§»æ•ˆæœè‰¯å¥½ï¼Œå·²æ¥è¿‘å•åŸŸä¸Šé™")
        logger.info("  - å¦‚æœè·¨åŸŸè¿ç§» F1 > Yelp-Only F1:")
        logger.info("    â†’ è¯´æ˜ Amazon æ•°æ®å¸¦æ¥äº†æ­£è¿ç§»å¢ç›Š")
        
        logger.info("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Yelp-Only å¯¹ç…§å®éªŒ - è¯„ä¼° BayesReviewNet å•åŸŸæ€§èƒ½ä¸Šé™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‡‡æ ·è®¾ç½®ï¼ˆé»˜è®¤ï¼‰
  python yelp_only_experiment.py
  
  # æŒ‡å®šé‡‡æ ·å¤§å°
  python yelp_only_experiment.py --sample-size 10000
  
  # ä½¿ç”¨å…¨é‡æ•°æ®ï¼ˆå¿½ç•¥é…ç½®æ–‡ä»¶ï¼‰
  python yelp_only_experiment.py --no-sampling
  
  # è‡ªå®šä¹‰æ•°æ®åˆ’åˆ†
  python yelp_only_experiment.py --sample-size 50000 --train-ratio 0.8 --val-ratio 0.1 --test-ratio 0.1
        """
    )
    
    # æ•°æ®é‡‡æ ·é€‰é¡¹
    sampling_group = parser.add_mutually_exclusive_group()
    sampling_group.add_argument(
        '--sample-size',
        type=int,
        default=None,
        help='é‡‡æ ·å¤§å°ï¼ˆæŒ‡å®šåä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    sampling_group.add_argument(
        '--no-sampling',
        action='store_true',
        help='ä¸ä½¿ç”¨é‡‡æ ·ï¼ŒåŠ è½½å…¨é‡æ•°æ®ï¼ˆä¼šè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰'
    )
    
    # æ•°æ®åˆ’åˆ†é€‰é¡¹
    parser.add_argument(
        '--train-ratio',
        type=float,
        default=0.70,
        help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.70ï¼‰'
    )
    parser.add_argument(
        '--val-ratio',
        type=float,
        default=0.15,
        help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼‰'
    )
    parser.add_argument(
        '--test-ratio',
        type=float,
        default=0.15,
        help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼‰'
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤configs/default.yamlï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šé‡‡æ ·ç­–ç•¥
    if args.no_sampling:
        use_sampling = False
        sample_size = None
        logger.info("æ¨¡å¼: å…¨é‡æ•°æ®ï¼ˆä¸é‡‡æ ·ï¼‰")
    elif args.sample_size is not None:
        use_sampling = True
        sample_size = args.sample_size
        logger.info(f"æ¨¡å¼: æŒ‡å®šé‡‡æ ·å¤§å° = {sample_size}")
    else:
        use_sampling = True
        sample_size = None
        logger.info("æ¨¡å¼: ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é‡‡æ ·è®¾ç½®")
    
    # è¿è¡Œå®éªŒ
    experiment = YelpOnlyExperiment(
        config_path=args.config,
        use_sampling=use_sampling
    )
    results = experiment.run(
        sample_size=sample_size,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.seed
    )
    
    logger.info("\nğŸ‰ Yelp-Only å¯¹ç…§å®éªŒå®Œæˆï¼")


if __name__ == '__main__':
    main()

