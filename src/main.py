#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸»æ‰§è¡Œè„šæœ¬ - Pipelineè°ƒåº¦å™¨
åè°ƒæ•´ä¸ªè´å¶æ–¯ç½‘ç»œå»ºæ¨¡æµç¨‹
"""
import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.config import load_config, ensure_dir
from utils.logging import setup_logger
from utils.io import save_data, save_metadata

# Preprocessing
from preprocessing import AmazonPreprocessor, YelpPreprocessor

# Features
from features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
from features.discretize import create_discretization_summary

# Bayes
from bayes import BayesianNetworkStructure, CPDLearner, BayesianInference

# Evaluation
from evaluation import evaluate_model

logger = setup_logger("main")


class BayesReviewNetPipeline:
    """
    è´å¶æ–¯è¯„è®ºç½‘ç»œPipeline
    
    å®Œæ•´æµç¨‹ï¼š
    1. æ•°æ®é¢„å¤„ç†ï¼ˆPreprocessingï¼‰- æ”¯æŒAmazonå’ŒYelpæ•°æ®é›†
    2. ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰- Text + Behavior + Networkå¤šè§†è§’ç‰¹å¾
    3. è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ï¼ˆBayesian Networkï¼‰- DAGç»“æ„ä¸CPDå­¦ä¹ 
    4. æ¨æ–­ä¸è¯„ä¼°ï¼ˆInference & Evaluationï¼‰
    """
    
    # æ”¯æŒçš„æ•°æ®é›†
    SUPPORTED_DATASETS = ['amazon', 'yelp']
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        logger.info("="*80)
        logger.info("BayesReviewNet Pipeline åˆå§‹åŒ–")
        logger.info("="*80)
    
    def run(self, dataset_name: str, structure_type: str = 'default') -> dict:
        """
        è¿è¡Œå®Œæ•´Pipeline
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ('amazon', 'yelp')
            structure_type: è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡å­—å…¸
        """
        if dataset_name not in self.SUPPORTED_DATASETS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}ã€‚æ”¯æŒçš„æ•°æ®é›†: {self.SUPPORTED_DATASETS}")
        
        logger.info(f"\nå¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}\n")
        
        # ========== é˜¶æ®µ1: æ•°æ®é¢„å¤„ç† ==========
        logger.info("ã€é˜¶æ®µ1ã€‘æ•°æ®é¢„å¤„ç†")
        df = self._preprocess(dataset_name)
        
        # ========== é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹ ==========
        logger.info("\nã€é˜¶æ®µ2ã€‘ç‰¹å¾å·¥ç¨‹ - æå–Text + Behavior + Networkç‰¹å¾")
        df = self._extract_features(df)
        
        # ========== é˜¶æ®µ2.5: æ„é€ å¼±æ ‡ç­¾ ==========
        logger.info("\nã€é˜¶æ®µ2.5ã€‘å¼±æ ‡ç­¾æ„é€ ")
        df = self._construct_weak_labels(df, dataset_name)
        
        # ========== é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ ==========
        logger.info("\nã€é˜¶æ®µ3ã€‘è´å¶æ–¯ç½‘ç»œå»ºæ¨¡")
        structure, cpd_learner = self._build_bayesian_network(df, structure_type)
        
        # ========== é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼° ==========
        logger.info("\nã€é˜¶æ®µ4ã€‘æ¨æ–­ä¸è¯„ä¼°")
        df = self._inference_and_evaluate(df, structure, cpd_learner, dataset_name)
        
        # ========== ä¿å­˜æœ€ç»ˆç»“æœ ==========
        output_path = self._save_results(df, dataset_name)
        
        # ========== ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ ==========
        stats = self._generate_statistics(df, dataset_name, output_path)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ•°æ®é›† {dataset_name} å¤„ç†å®Œæˆï¼")
        logger.info(f"{'='*80}\n")
        
        return stats
    
    def _preprocess(self, dataset_name: str):
        """
        é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†
        
        æ‰€æœ‰æ•°æ®é›†ç»Ÿä¸€è¾“å‡ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„DataFrame:
        - user_id, item_id, review_id, timestamp, rating, review_text
        - platform, verified, vote
        - weak_label, label_source
        """
        if dataset_name == 'amazon':
            preprocessor = AmazonPreprocessor(
                self.config['data_paths']['amazon']['raw_dir']
            )
            sample_size = self.config['sampling']['amazon_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        elif dataset_name == 'yelp':
            preprocessor = YelpPreprocessor(
                self.config['data_paths']['yelp']['raw_dir']
            )
            sample_size = self.config['sampling']['yelp_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        save_data(df, f"{output_dir}/{dataset_name}_standardized.parquet")
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(df)} æ¡è®°å½•")
        return df
    
    def _extract_features(self, df):
        """
        é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹
        
        æå–å¤šè§†è§’ç‰¹å¾:
        - Textç‰¹å¾: æ–‡æœ¬ç»Ÿè®¡ã€æƒ…æ„Ÿã€ä¸»è§‚æ€§ç­‰
        - Behaviorç‰¹å¾: ç”¨æˆ·è¯„è®ºæ•°ã€è¯„åˆ†æ¨¡å¼ã€æ—¶é—´æ¨¡å¼ç­‰
        - Networkç‰¹å¾: ç”¨æˆ·-å•†å“å›¾ç»“æ„ç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        """
        # 2.1 æ–‡æœ¬ç‰¹å¾
        logger.info("  â†’ æå–Textç‰¹å¾...")
        text_extractor = TextFeatureExtractor()
        df = text_extractor.extract(df)
        
        # 2.2 è¡Œä¸ºç‰¹å¾
        logger.info("  â†’ æå–Behaviorç‰¹å¾...")
        behavior_extractor = BehaviorFeatureExtractor()
        df = behavior_extractor.extract(df)
        
        # TODO: 2.3 ç½‘ç»œç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        # logger.info("  â†’ æå–Networkç‰¹å¾...")
        # network_extractor = NetworkFeatureExtractor()
        # df = network_extractor.extract(df)
        
        # 2.4 ç‰¹å¾ç¦»æ•£åŒ–ï¼ˆæ•°æ®é©±åŠ¨çš„åˆ†ä½æ•°ç¦»æ•£åŒ–ï¼‰
        logger.info("  â†’ ç‰¹å¾ç¦»æ•£åŒ–ï¼ˆåŸºäºåˆ†ä½æ•°ï¼‰...")
        discretizer = FeatureDiscretizer()  # ä¸å†éœ€è¦configå‚æ•°
        df = discretizer.discretize(df)
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len([c for c in df.columns if '_discrete' in c])} ä¸ªç¦»æ•£ç‰¹å¾")
        return df
    
    def _construct_weak_labels(self, df, dataset_name: str):
        """
        é˜¶æ®µ2.5: æ„é€ å¼±æ ‡ç­¾
        
        åŸºäºå¯å‘å¼è§„åˆ™æˆ–å¹³å°ä¿¡å·æ„é€ å¼±ç›‘ç£æ ‡ç­¾
        å¿…é¡»åœ¨ç‰¹å¾æå–ä¹‹åã€è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ä¹‹å‰æ‰§è¡Œ
        """
        from src.preprocessing.weak_labeling import construct_weak_label
        
        platform = df['platform'].iloc[0] if 'platform' in df.columns else dataset_name
        df = construct_weak_label(df, platform)
        
        return df
    
    def _build_bayesian_network(self, df, structure_type: str):
        """
        é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡
        
        åŸºäºå¤šè§†è§’ç‰¹å¾æ„å»ºè´å¶æ–¯ç½‘ç»œ
        """
        # 3.1 å®šä¹‰DAGç»“æ„
        structure = BayesianNetworkStructure()
        structure.define_structure(structure_type)
        
        logger.info(f"DAGç»“æ„å·²å®šä¹‰: {len(structure.edges)} æ¡è¾¹")
        logger.info(f"æ‹“æ‰‘æ’åº: {structure.get_topological_order()}")
        
        # 3.2 å­¦ä¹ CPD
        cpd_learner = CPDLearner(structure)
        cpd_learner.learn_cpds(df, smoothing=1.0)
        
        return structure, cpd_learner
    
    def _inference_and_evaluate(self, df, structure, cpd_learner, dataset_name: str):
        """
        é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼°
        
        ä½¿ç”¨è´å¶æ–¯æ¨æ–­è®¡ç®—åéªŒæ¦‚ç‡
        """
        # 4.1 è´å¶æ–¯æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        df = inference.infer_posterior(df, target_variable='weak_label')
        
        # 4.2 å¦‚æœæœ‰å¼±æ ‡ç­¾ï¼Œå¯ä»¥è¿›è¡Œè¯„ä¼°
        if 'weak_label' in df.columns and df['weak_label'].notna().any():
            logger.info("\nè¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            try:
                evaluation_result = evaluate_model(df)
                logger.info(f"  Precision: {evaluation_result['metrics']['precision']:.4f}")
                logger.info(f"  Recall: {evaluation_result['metrics']['recall']:.4f}")
                logger.info(f"  F1-Score: {evaluation_result['metrics']['f1']:.4f}")
                logger.info(f"  ROC-AUC: {evaluation_result['metrics'].get('roc_auc', 'N/A')}")
            except Exception as e:
                logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
        
        return df
    
    def _save_results(self, df, dataset_name: str) -> str:
        """
        ä¿å­˜æœ€ç»ˆç»“æœåˆ°data/processed/ç›®å½•
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        
        # ä¿å­˜æœ€ç»ˆå¤„ç†ç»“æœ
        output_path = f"{output_dir}/{dataset_name}_final.parquet"
        save_data(df, output_path)
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")
        return output_path
    
    def _generate_statistics(self, df, dataset_name: str, output_path: str) -> dict:
        """
        ç”Ÿæˆæ•°æ®é›†å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'dataset': dataset_name,
            'total_samples': len(df),
            'output_file': output_path,
            'features': {}
        }
        
        # ç»Ÿè®¡æ–‡æœ¬ç‰¹å¾
        text_features = [col for col in df.columns if col in [
            'review_length', 'sentiment_score', 'subjectivity_score',
            'exclamation_ratio', 'first_person_pronoun_ratio'
        ]]
        stats['features']['text'] = len(text_features)
        
        # ç»Ÿè®¡è¡Œä¸ºç‰¹å¾
        behavior_features = [col for col in df.columns if col.startswith('user_')]
        stats['features']['behavior'] = len(behavior_features)
        
        # ç»Ÿè®¡ç¦»æ•£åŒ–ç‰¹å¾
        discrete_features = [col for col in df.columns if col.endswith('_discrete')]
        stats['features']['discrete'] = len(discrete_features)
        
        # ç»Ÿè®¡å¼±æ ‡ç­¾åˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'weak_label' in df.columns:
            label_dist = df['weak_label'].value_counts().to_dict()
            stats['weak_label_distribution'] = {
                'suspicious': int(label_dist.get(1, 0)),
                'normal': int(label_dist.get(0, 0)),
                'missing': int(df['weak_label'].isna().sum())
            }
        
        # ç»Ÿè®¡åéªŒæ¦‚ç‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'weak_label_posterior_prob' in df.columns:
            posterior = df['weak_label_posterior_prob'].dropna()
            if len(posterior) > 0:
                stats['posterior_prob'] = {
                    'mean': float(posterior.mean()),
                    'median': float(posterior.median()),
                    'max': float(posterior.max()),
                    'samples_with_prob': len(posterior)
                }
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='BayesReviewNet - åŸºäºè´å¶æ–¯ç½‘ç»œçš„è™šå‡è¯„è®ºè¯†åˆ«ï¼ˆå¤šè§†è§’ç‰¹å¾ï¼‰'
    )
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--datasets',
        nargs='+',
        choices=['amazon', 'yelp', 'all'],
        default=['all'],
        help='è¦å¤„ç†çš„æ•°æ®é›† (amazon, yelp)'
    )
    parser.add_argument(
        '--structure',
        type=str,
        choices=['default', 'naive'],
        default='default',
        help='è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹'
    )
    parser.add_argument(
        '--cross-domain',
        action='store_true',
        help='å¯ç”¨è·¨åŸŸè¿ç§»æ¨¡å¼ï¼ˆAmazonâ†’Yelpï¼Œä½¿ç”¨ä¼¼ç„¶æ ¡å‡†ï¼‰'
    )
    parser.add_argument(
        '--use-validation',
        action='store_true',
        default=True,
        help='ä½¿ç”¨é¢„å®šä¹‰çš„éªŒè¯é›†è¿›è¡Œæ ¡å‡†ï¼ˆé»˜è®¤Trueï¼‰'
    )
    parser.add_argument(
        '--calibration-ratio',
        type=float,
        default=0.20,
        help='æ ¡å‡†é›†æ¯”ä¾‹ï¼ˆä»…åœ¨--no-use-validationæ—¶ç”Ÿæ•ˆï¼Œé»˜è®¤0.20ï¼‰'
    )
    parser.add_argument(
        '--calibration-strength',
        type=float,
        default=0.3,
        help='æ ¡å‡†å¼ºåº¦Î±ï¼ˆé»˜è®¤0.3ï¼ŒèŒƒå›´0-1ï¼‰'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœå¯ç”¨è·¨åŸŸè¿ç§»æ¨¡å¼ï¼Œä½¿ç”¨ä¸“é—¨çš„pipeline
    if args.cross_domain:
        logger.info("\n" + "="*80)
        logger.info("è·¨åŸŸè¿ç§»æ¨¡å¼ (Cross-Domain Transfer Mode)")
        logger.info("="*80)
        _run_cross_domain_transfer(args)
        return
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if 'all' in args.datasets:
        datasets = ['amazon', 'yelp']
    else:
        datasets = args.datasets
    
    # åˆå§‹åŒ–Pipeline
    pipeline = BayesReviewNetPipeline(args.config)
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    results = {}
    for dataset_name in datasets:
        try:
            stats = pipeline.run(dataset_name, args.structure)
            results[dataset_name] = stats
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}", exc_info=True)
            results[dataset_name] = {'status': 'failed', 'error': str(e)}
            continue
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    _print_summary(results)
    
    logger.info("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


def _run_cross_domain_transfer(args):
    """
    è¿è¡Œè·¨åŸŸè¿ç§»å­¦ä¹ 
    
    Amazon (æºåŸŸ) â†’ Yelp (ç›®æ ‡åŸŸ) + ä¼¼ç„¶æ ¡å‡†
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    from utils.config import load_config
    from utils.data_split import split_for_calibration, validate_split
    from preprocessing import AmazonPreprocessor, YelpPreprocessor
    from preprocessing.weak_labeling import construct_weak_label
    from features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
    from bayes import BayesianNetworkStructure, CPDLearner, BayesianInference, LikelihoodCalibrator
    from evaluation import evaluate_model
    
    config = load_config(args.config)
    
    logger.info(f"é…ç½®:")
    logger.info(f"  - ä½¿ç”¨éªŒè¯é›†: {args.use_validation}")
    if not args.use_validation:
        logger.info(f"  - æ ¡å‡†é›†æ¯”ä¾‹: {args.calibration_ratio*100:.0f}%")
    logger.info(f"  - æ ¡å‡†å¼ºåº¦ Î±: {args.calibration_strength}")
    logger.info(f"  - ç½‘ç»œç»“æ„: {args.structure}")
    
    # ========== æ­¥éª¤1: å‡†å¤‡AmazonæºåŸŸæ•°æ® ==========
    logger.info("\nã€æ­¥éª¤1ã€‘å‡†å¤‡AmazonæºåŸŸæ•°æ®")
    amazon_preprocessor = AmazonPreprocessor(config['data_paths']['amazon']['raw_dir'])
    amazon_sample_size = config['sampling']['amazon_sample_size'] \
        if config['sampling']['enabled'] else None
    amazon_df = amazon_preprocessor.load_and_standardize(amazon_sample_size)
    
    # ç‰¹å¾æå–
    logger.info("  â†’ æå–ç‰¹å¾...")
    amazon_df = _extract_all_features(amazon_df)
    amazon_df = construct_weak_label(amazon_df, 'amazon')
    logger.info(f"  âœ“ Amazonæ•°æ®: {len(amazon_df)} æ¡")
    
    # ========== æ­¥éª¤2: åœ¨Amazonä¸Šè®­ç»ƒè´å¶æ–¯ç½‘ç»œ ==========
    logger.info("\nã€æ­¥éª¤2ã€‘åœ¨Amazonä¸Šè®­ç»ƒè´å¶æ–¯ç½‘ç»œï¼ˆæºåŸŸï¼‰")
    structure = BayesianNetworkStructure()
    structure.define_structure(args.structure)
    logger.info(f"  âœ“ DAGç»“æ„: {len(structure.edges)} æ¡è¾¹")
    
    amazon_cpd = CPDLearner(structure)
    amazon_cpd.learn_cpds(amazon_df, smoothing=1.0)
    logger.info(f"  âœ“ CPDå­¦ä¹ å®Œæˆï¼ˆæºåŸŸçŸ¥è¯†ï¼‰")
    
    # ========== æ­¥éª¤3: å‡†å¤‡Yelpç›®æ ‡åŸŸæ•°æ®å¹¶åˆ’åˆ† ==========
    logger.info("\nã€æ­¥éª¤3ã€‘å‡†å¤‡Yelpç›®æ ‡åŸŸæ•°æ®å¹¶åˆ’åˆ†")
    yelp_preprocessor = YelpPreprocessor(config['data_paths']['yelp']['raw_dir'])
    
    if args.use_validation:
        # ä½¿ç”¨å›ºå®šçš„éªŒè¯é›†åˆ’åˆ†ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶åˆ›å»ºï¼Œåç»­é‡ç”¨ï¼‰
        logger.info("  â†’ ä½¿ç”¨å›ºå®šçš„éªŒè¯é›†åˆ’åˆ†ï¼ˆç¡®ä¿å¯é‡å¤æ€§ï¼‰")
        
        yelp_calib, yelp_test = _load_or_create_fixed_split(
            yelp_preprocessor,
            config,
            calibration_ratio=args.calibration_ratio
        )
        
        logger.info(f"  âœ“ éªŒè¯é›†ï¼ˆæ ¡å‡†ç”¨ï¼‰: {len(yelp_calib)} æ¡")
        logger.info(f"  âœ“ æµ‹è¯•é›†: {len(yelp_test)} æ¡")
        
    else:
        # éšæœºåˆ’åˆ†æ–¹å¼ï¼ˆæ¯æ¬¡è¿è¡Œé‡æ–°åˆ’åˆ†ï¼‰
        yelp_sample_size = config['sampling']['yelp_sample_size'] \
            if config['sampling']['enabled'] else None
        yelp_df = yelp_preprocessor.load_and_standardize(yelp_sample_size)
        
        # ç‰¹å¾æå–
        logger.info("  â†’ æå–ç‰¹å¾...")
        yelp_df = _extract_all_features(yelp_df)
        yelp_df = construct_weak_label(yelp_df, 'yelp')
        logger.info(f"  âœ“ Yelpæ•°æ®: {len(yelp_df)} æ¡")
        
        # åˆ’åˆ†ä¸ºæ ¡å‡†é›†å’Œæµ‹è¯•é›†
        logger.info(f"  â†’ éšæœºåˆ’åˆ†æ•°æ®ï¼ˆæ ¡å‡†:{args.calibration_ratio*100:.0f}% / æµ‹è¯•:{(1-args.calibration_ratio)*100:.0f}%ï¼‰")
        yelp_calib, yelp_test = split_for_calibration(
            yelp_df,
            calibration_ratio=args.calibration_ratio,
            stratify_by='weak_label',
            random_state=42
        )
        validate_split(yelp_calib, yelp_test, label_col='weak_label')
    
    # ========== æ­¥éª¤4: æ‰§è¡Œä¼¼ç„¶æ ¡å‡† ==========
    logger.info(f"\nã€æ­¥éª¤4ã€‘æ‰§è¡Œä¼¼ç„¶æ ¡å‡†ï¼ˆÎ±={args.calibration_strength}ï¼‰")
    calibrator = LikelihoodCalibrator(
        amazon_cpd,
        calibration_strength=args.calibration_strength
    )
    calibrator.calibrate(yelp_calib, target_variable='weak_label')
    
    calibrated_cpd = calibrator.get_calibrated_cpd_learner()
    calib_report = calibrator.get_calibration_report()
    logger.info(f"  âœ“ æ ¡å‡†å®Œæˆ:")
    logger.info(f"    - æ€»èŠ‚ç‚¹: {calib_report['total_nodes']}")
    logger.info(f"    - å·²æ ¡å‡†: {calib_report['calibrated_nodes']}")
    logger.info(f"    - ä¿æŒä¸å˜: {calib_report['kept_nodes']}")
    
    # ========== æ­¥éª¤5: åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ==========
    logger.info("\nã€æ­¥éª¤5ã€‘åœ¨Yelpæµ‹è¯•é›†ä¸Šè¯„ä¼°")
    
    # 5a. åŸºçº¿ï¼ˆæ— æ ¡å‡†ï¼‰
    logger.info("  â†’ åŸºçº¿æ€§èƒ½ï¼ˆæ— æ ¡å‡†ï¼‰:")
    baseline_results = _evaluate_on_test(yelp_test, structure, amazon_cpd)
    
    # 5b. æ ¡å‡†å
    logger.info("\n  â†’ æ ¡å‡†åæ€§èƒ½:")
    calibrated_results = _evaluate_on_test(yelp_test, structure, calibrated_cpd)
    
    # ========== æ­¥éª¤6: å¯¹æ¯”åˆ†æ ==========
    logger.info("\nã€æ­¥éª¤6ã€‘æ€§èƒ½å¯¹æ¯”åˆ†æ")
    _compare_performance(baseline_results, calibrated_results)
    
    # ========== æ­¥éª¤7: ä¿å­˜ç»“æœ ==========
    logger.info("\nã€æ­¥éª¤7ã€‘ä¿å­˜ç»“æœ")
    output_path = f"data/processed/yelp_calibrated_r{int(args.calibration_ratio*100)}_a{int(args.calibration_strength*100)}.parquet"
    
    # åœ¨æ ¡å‡†åçš„CPDä¸Šè¿›è¡Œæ¨æ–­
    inference = BayesianInference(structure, calibrated_cpd)
    yelp_test = inference.infer_posterior(yelp_test, target_variable='weak_label')
    
    from utils.io import save_data
    save_data(yelp_test, output_path)
    logger.info(f"  âœ“ ç»“æœå·²ä¿å­˜: {output_path}")
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ è·¨åŸŸè¿ç§»å­¦ä¹ å®Œæˆï¼")
    logger.info("="*80)


def _load_or_create_fixed_split(yelp_preprocessor, config, calibration_ratio=0.20):
    """
    åŠ è½½æˆ–åˆ›å»ºå›ºå®šçš„YelpéªŒè¯é›†/æµ‹è¯•é›†åˆ’åˆ†
    
    é¦–æ¬¡è¿è¡Œæ—¶åˆ›å»ºåˆ’åˆ†å¹¶ä¿å­˜ç´¢å¼•ï¼Œåç»­è¿è¡Œé‡ç”¨ç›¸åŒçš„åˆ’åˆ†
    è¿™ç¡®ä¿äº†è·¨åŸŸå®éªŒçš„å¯é‡å¤æ€§
    
    Args:
        yelp_preprocessor: Yelpé¢„å¤„ç†å™¨
        config: é…ç½®å­—å…¸
        calibration_ratio: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        (validation_df, test_df)
    """
    from pathlib import Path
    import pickle
    from preprocessing.weak_labeling import construct_weak_label
    
    # åˆ’åˆ†ç´¢å¼•æ–‡ä»¶è·¯å¾„
    split_file = Path('data/processed/yelp_fixed_split_indices.pkl')
    
    # åŠ è½½å®Œæ•´çš„Yelpæ•°æ®
    yelp_sample_size = config['sampling']['yelp_sample_size'] \
        if config['sampling']['enabled'] else None
    yelp_df = yelp_preprocessor.load_and_standardize(yelp_sample_size)
    
    # ç‰¹å¾æå–ï¼ˆåœ¨åˆ’åˆ†ä¹‹å‰ï¼‰
    logger.info("  â†’ æå–ç‰¹å¾...")
    yelp_df = _extract_all_features(yelp_df)
    yelp_df = construct_weak_label(yelp_df, 'yelp')
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å›ºå®šåˆ’åˆ†
    if split_file.exists():
        logger.info(f"  â†’ åŠ è½½å›ºå®šåˆ’åˆ†ç´¢å¼•: {split_file}")
        with open(split_file, 'rb') as f:
            split_indices = pickle.load(f)
        
        val_indices = split_indices['validation']
        test_indices = split_indices['test']
        
        # ä½¿ç”¨ä¿å­˜çš„ç´¢å¼•åˆ’åˆ†æ•°æ®
        validation_df = yelp_df.iloc[val_indices].copy()
        test_df = yelp_df.iloc[test_indices].copy()
        
        logger.info(f"  âœ“ ä½¿ç”¨å›ºå®šåˆ’åˆ†ï¼ˆéªŒè¯é›†:{len(validation_df)}, æµ‹è¯•é›†:{len(test_df)}ï¼‰")
        
    else:
        logger.info(f"  â†’ åˆ›å»ºæ–°çš„å›ºå®šåˆ’åˆ†ï¼ˆéªŒè¯é›†:{calibration_ratio*100:.0f}%ï¼‰")
        
        # åˆ›å»ºæ–°åˆ’åˆ†
        from utils.data_split import split_for_calibration, validate_split
        
        validation_df, test_df = split_for_calibration(
            yelp_df,
            calibration_ratio=calibration_ratio,
            stratify_by='weak_label',
            random_state=42
        )
        
        # ä¿å­˜ç´¢å¼•ä»¥ä¾›åç»­ä½¿ç”¨
        split_indices = {
            'validation': validation_df.index.tolist(),
            'test': test_df.index.tolist(),
            'calibration_ratio': calibration_ratio,
            'total_samples': len(yelp_df)
        }
        
        split_file.parent.mkdir(parents=True, exist_ok=True)
        with open(split_file, 'wb') as f:
            pickle.dump(split_indices, f)
        
        logger.info(f"  âœ“ å›ºå®šåˆ’åˆ†å·²ä¿å­˜: {split_file}")
        
        # éªŒè¯åˆ’åˆ†
        validate_split(validation_df, test_df, label_col='weak_label')
    
    return validation_df, test_df


def _extract_all_features(df):
    """è¾…åŠ©å‡½æ•°ï¼šæå–æ‰€æœ‰ç‰¹å¾"""
    text_extractor = TextFeatureExtractor()
    df = text_extractor.extract(df)
    
    behavior_extractor = BehaviorFeatureExtractor()
    df = behavior_extractor.extract(df)
    
    discretizer = FeatureDiscretizer()
    df = discretizer.discretize(df)
    
    return df


def _evaluate_on_test(test_df, structure, cpd_learner):
    """è¾…åŠ©å‡½æ•°ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
    from evaluation.metrics import find_optimal_threshold
    
    inference = BayesianInference(structure, cpd_learner)
    test_df = inference.infer_posterior(test_df.copy(), target_variable='weak_label')
    
    # åéªŒæ¦‚ç‡ç»Ÿè®¡
    if 'weak_label_posterior_prob' in test_df.columns:
        posterior = test_df['weak_label_posterior_prob'].dropna()
        if len(posterior) > 0:
            logger.info(f"    åéªŒå‡å€¼: {posterior.mean():.4f}")
            logger.info(f"    åéªŒä¸­ä½æ•°: {posterior.median():.4f}")
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    optimal_result = find_optimal_threshold(test_df, metric='f1')
    optimal_threshold = optimal_result['best_threshold']
    logger.info(f"    æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¯„ä¼°
    results = evaluate_model(test_df, threshold=optimal_threshold)
    metrics = results['metrics']
    
    logger.info(f"    Precision: {metrics['precision']:.4f}")
    logger.info(f"    Recall:    {metrics['recall']:.4f}")
    logger.info(f"    F1-Score:  {metrics['f1']:.4f}")
    logger.info(f"    ROC-AUC:   {metrics.get('roc_auc', 'N/A')}")
    
    return results


def _compare_performance(baseline, calibrated):
    """è¾…åŠ©å‡½æ•°ï¼šå¯¹æ¯”æ€§èƒ½"""
    logger.info("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    logger.info("  â”‚   æŒ‡æ ‡     â”‚ Baseline â”‚ Calibrated â”‚  æå‡   â”‚")
    logger.info("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    metrics = ['precision', 'recall', 'f1', 'roc_auc']
    metric_names = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    
    for metric, name in zip(metrics, metric_names):
        base_val = baseline['metrics'].get(metric, 0.0)
        calib_val = calibrated['metrics'].get(metric, 0.0)
        improvement = calib_val - base_val
        
        improvement_str = f"+{improvement:.4f}" if improvement >= 0 else f"{improvement:.4f}"
        
        logger.info(
            f"  â”‚ {name:10s} â”‚ {base_val:8.4f} â”‚  {calib_val:8.4f}  â”‚ {improvement_str:7s} â”‚"
        )
    
    logger.info("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # æ€»ç»“
    f1_improvement = calibrated['metrics']['f1'] - baseline['metrics']['f1']
    if f1_improvement > 0.01:
        logger.info(f"\n  âœ“ æ ¡å‡†æœ‰æ•ˆï¼F1-Scoreæå‡ {f1_improvement:.4f}")
    elif f1_improvement > 0:
        logger.info(f"\n  â†’ æ ¡å‡†ç•¥æœ‰æ”¹å–„ï¼ŒF1-Scoreæå‡ {f1_improvement:.4f}")
    else:
        logger.info(f"\n  âš  æ ¡å‡†æœªå¸¦æ¥æ˜¾è‘—æå‡ï¼ŒF1-Scoreå˜åŒ– {f1_improvement:.4f}")


def _print_summary(results: dict):
    """
    æ‰“å°å¤„ç†ç»“æœæ±‡æ€»
    
    Args:
        results: å„æ•°æ®é›†çš„å¤„ç†ç»“æœå­—å…¸
    """
    logger.info("\n" + "="*80)
    logger.info("å¤„ç†ç»“æœæ±‡æ€»")
    logger.info("="*80)
    
    for dataset_name, stats in results.items():
        if stats.get('status') == 'failed':
            logger.info(f"\nâŒ {dataset_name.upper()}: å¤„ç†å¤±è´¥")
            logger.info(f"   é”™è¯¯: {stats.get('error', 'Unknown')}")
            continue
        
        logger.info(f"\nâœ… {dataset_name.upper()}")
        logger.info(f"   æ ·æœ¬æ•°: {stats['total_samples']:,}")
        logger.info(f"   ç‰¹å¾ç»Ÿè®¡:")
        logger.info(f"      - Textç‰¹å¾: {stats['features']['text']} ä¸ª")
        logger.info(f"      - Behaviorç‰¹å¾: {stats['features']['behavior']} ä¸ª")
        logger.info(f"      - ç¦»æ•£åŒ–ç‰¹å¾: {stats['features']['discrete']} ä¸ª")
        
        # å¼±æ ‡ç­¾åˆ†å¸ƒ
        if 'weak_label_distribution' in stats:
            dist = stats['weak_label_distribution']
            total_labeled = dist['suspicious'] + dist['normal']
            if total_labeled > 0:
                susp_rate = dist['suspicious'] / total_labeled * 100
                logger.info(f"   å¼±æ ‡ç­¾åˆ†å¸ƒ:")
                logger.info(f"      - å¯ç–‘: {dist['suspicious']:,} ({susp_rate:.1f}%)")
                logger.info(f"      - æ­£å¸¸: {dist['normal']:,} ({100-susp_rate:.1f}%)")
                if dist['missing'] > 0:
                    logger.info(f"      - ç¼ºå¤±: {dist['missing']:,}")
        
        # åéªŒæ¦‚ç‡ç»Ÿè®¡
        if 'posterior_prob' in stats:
            post = stats['posterior_prob']
            logger.info(f"   åéªŒæ¦‚ç‡:")
            logger.info(f"      - å¹³å‡: {post['mean']:.4f}")
            logger.info(f"      - ä¸­ä½æ•°: {post['median']:.4f}")
            logger.info(f"      - æœ€å¤§å€¼: {post['max']:.4f}")
            logger.info(f"      - æœ‰æ•ˆæ ·æœ¬: {post['samples_with_prob']:,}")
        
        logger.info(f"   è¾“å‡ºæ–‡ä»¶: {stats['output_file']}")
    
    logger.info("\n" + "="*80)


if __name__ == '__main__':
    main()
