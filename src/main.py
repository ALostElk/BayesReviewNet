#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸»æ‰§è¡Œè„šæœ¬ - Pipelineè°ƒåº¦å™¨
åè°ƒæ•´ä¸ªè´å¶æ–¯ç½‘ç»œå»ºæ¨¡æµç¨‹
"""
import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.config import load_config, ensure_dir
from utils.logging import setup_logger
from utils.io import save_data, save_metadata

# Preprocessing
from preprocessing import AmazonPreprocessor, YelpPreprocessor

# Features
from features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
from features.discretize import create_discretization_summary

# Bayes
from bayes import BayesianNetworkStructure, CPDLearner, BayesianInference

# Evaluation
from evaluation import evaluate_model

logger = setup_logger("main")


class BayesReviewNetPipeline:
    """
    è´å¶æ–¯è¯„è®ºç½‘ç»œPipeline
    
    å®Œæ•´æµç¨‹ï¼š
    1. æ•°æ®é¢„å¤„ç†ï¼ˆPreprocessingï¼‰- æ”¯æŒAmazonå’ŒYelpæ•°æ®é›†
    2. ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰- Text + Behavior + Networkå¤šè§†è§’ç‰¹å¾
    3. è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ï¼ˆBayesian Networkï¼‰- DAGç»“æ„ä¸CPDå­¦ä¹ 
    4. æ¨æ–­ä¸è¯„ä¼°ï¼ˆInference & Evaluationï¼‰
    """
    
    # æ”¯æŒçš„æ•°æ®é›†
    SUPPORTED_DATASETS = ['amazon', 'yelp']
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        logger.info("="*80)
        logger.info("BayesReviewNet Pipeline åˆå§‹åŒ–")
        logger.info("="*80)
    
    def run(self, dataset_name: str, structure_type: str = 'default'):
        """
        è¿è¡Œå®Œæ•´Pipeline
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ('amazon', 'yelp')
            structure_type: è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹
        """
        if dataset_name not in self.SUPPORTED_DATASETS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}ã€‚æ”¯æŒçš„æ•°æ®é›†: {self.SUPPORTED_DATASETS}")
        
        logger.info(f"\nå¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}\n")
        
        # ========== é˜¶æ®µ1: æ•°æ®é¢„å¤„ç† ==========
        logger.info("ã€é˜¶æ®µ1ã€‘æ•°æ®é¢„å¤„ç†")
        df = self._preprocess(dataset_name)
        
        # ========== é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹ ==========
        logger.info("\nã€é˜¶æ®µ2ã€‘ç‰¹å¾å·¥ç¨‹ - æå–Text + Behavior + Networkç‰¹å¾")
        df = self._extract_features(df)
        
        # ========== é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ ==========
        logger.info("\nã€é˜¶æ®µ3ã€‘è´å¶æ–¯ç½‘ç»œå»ºæ¨¡")
        structure, cpd_learner = self._build_bayesian_network(df, structure_type)
        
        # ========== é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼° ==========
        logger.info("\nã€é˜¶æ®µ4ã€‘æ¨æ–­ä¸è¯„ä¼°")
        df = self._inference_and_evaluate(df, structure, cpd_learner, dataset_name)
        
        # ========== ä¿å­˜æœ€ç»ˆç»“æœ ==========
        self._save_results(df, dataset_name)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ•°æ®é›† {dataset_name} å¤„ç†å®Œæˆï¼")
        logger.info(f"{'='*80}\n")
    
    def _preprocess(self, dataset_name: str):
        """
        é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†
        
        æ‰€æœ‰æ•°æ®é›†ç»Ÿä¸€è¾“å‡ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„DataFrame:
        - user_id, item_id, review_id, timestamp, rating, review_text
        - platform, verified, vote
        - weak_label, label_source
        """
        if dataset_name == 'amazon':
            preprocessor = AmazonPreprocessor(
                self.config['data_paths']['amazon']['raw_dir']
            )
            sample_size = self.config['sampling']['amazon_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        elif dataset_name == 'yelp':
            preprocessor = YelpPreprocessor(
                self.config['data_paths']['yelp']['raw_dir']
            )
            sample_size = self.config['sampling']['yelp_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        save_data(df, f"{output_dir}/{dataset_name}_standardized.parquet")
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(df)} æ¡è®°å½•")
        return df
    
    def _extract_features(self, df):
        """
        é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹
        
        æå–å¤šè§†è§’ç‰¹å¾:
        - Textç‰¹å¾: æ–‡æœ¬ç»Ÿè®¡ã€æƒ…æ„Ÿã€ä¸»è§‚æ€§ç­‰
        - Behaviorç‰¹å¾: ç”¨æˆ·è¯„è®ºæ•°ã€è¯„åˆ†æ¨¡å¼ã€æ—¶é—´æ¨¡å¼ç­‰
        - Networkç‰¹å¾: ç”¨æˆ·-å•†å“å›¾ç»“æ„ç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        """
        # 2.1 æ–‡æœ¬ç‰¹å¾
        logger.info("  â†’ æå–Textç‰¹å¾...")
        text_extractor = TextFeatureExtractor()
        df = text_extractor.extract(df)
        
        # 2.2 è¡Œä¸ºç‰¹å¾
        logger.info("  â†’ æå–Behaviorç‰¹å¾...")
        behavior_extractor = BehaviorFeatureExtractor()
        df = behavior_extractor.extract(df)
        
        # TODO: 2.3 ç½‘ç»œç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        # logger.info("  â†’ æå–Networkç‰¹å¾...")
        # network_extractor = NetworkFeatureExtractor()
        # df = network_extractor.extract(df)
        
        # 2.4 ç‰¹å¾ç¦»æ•£åŒ–
        logger.info("  â†’ ç‰¹å¾ç¦»æ•£åŒ–...")
        discretizer = FeatureDiscretizer(self.config['discretization'])
        df = discretizer.discretize(df)
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len([c for c in df.columns if '_discrete' in c])} ä¸ªç¦»æ•£ç‰¹å¾")
        return df
    
    def _build_bayesian_network(self, df, structure_type: str):
        """
        é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡
        
        åŸºäºå¤šè§†è§’ç‰¹å¾æ„å»ºè´å¶æ–¯ç½‘ç»œ
        """
        # 3.1 å®šä¹‰DAGç»“æ„
        structure = BayesianNetworkStructure()
        structure.define_structure(structure_type)
        
        logger.info(f"DAGç»“æ„å·²å®šä¹‰: {len(structure.edges)} æ¡è¾¹")
        logger.info(f"æ‹“æ‰‘æ’åº: {structure.get_topological_order()}")
        
        # 3.2 å­¦ä¹ CPD
        cpd_learner = CPDLearner(structure)
        cpd_learner.learn_cpds(df, smoothing=1.0)
        
        return structure, cpd_learner
    
    def _inference_and_evaluate(self, df, structure, cpd_learner, dataset_name: str):
        """
        é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼°
        
        ä½¿ç”¨è´å¶æ–¯æ¨æ–­è®¡ç®—åéªŒæ¦‚ç‡
        """
        # 4.1 è´å¶æ–¯æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        df = inference.infer_posterior(df, target_variable='weak_label')
        
        # 4.2 å¦‚æœæœ‰å¼±æ ‡ç­¾ï¼Œå¯ä»¥è¿›è¡Œè¯„ä¼°
        if 'weak_label' in df.columns and df['weak_label'].notna().any():
            logger.info("\nè¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            try:
                evaluation_result = evaluate_model(df)
                logger.info(f"  Precision: {evaluation_result['metrics']['precision']:.4f}")
                logger.info(f"  Recall: {evaluation_result['metrics']['recall']:.4f}")
                logger.info(f"  F1-Score: {evaluation_result['metrics']['f1']:.4f}")
                logger.info(f"  ROC-AUC: {evaluation_result['metrics'].get('roc_auc', 'N/A')}")
            except Exception as e:
                logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
        
        return df
    
    def _save_results(self, df, dataset_name: str):
        """ä¿å­˜æœ€ç»ˆç»“æœåˆ°data/processed/ç›®å½•"""
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        
        # ä¿å­˜æœ€ç»ˆå¤„ç†ç»“æœ
        save_data(df, f"{output_dir}/{dataset_name}_final.parquet")
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='BayesReviewNet - åŸºäºè´å¶æ–¯ç½‘ç»œçš„è™šå‡è¯„è®ºè¯†åˆ«ï¼ˆå¤šè§†è§’ç‰¹å¾ï¼‰'
    )
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--datasets',
        nargs='+',
        choices=['amazon', 'yelp', 'all'],
        default=['all'],
        help='è¦å¤„ç†çš„æ•°æ®é›† (amazon, yelp)'
    )
    parser.add_argument(
        '--structure',
        type=str,
        choices=['default', 'naive'],
        default='default',
        help='è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if 'all' in args.datasets:
        datasets = ['amazon', 'yelp']
    else:
        datasets = args.datasets
    
    # åˆå§‹åŒ–Pipeline
    pipeline = BayesReviewNetPipeline(args.config)
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset_name in datasets:
        try:
            pipeline.run(dataset_name, args.structure)
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}", exc_info=True)
            continue
    
    logger.info("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    main()
