#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸»æ‰§è¡Œè„šæœ¬ - Pipelineè°ƒåº¦å™¨
åè°ƒæ•´ä¸ªè´å¶æ–¯ç½‘ç»œå»ºæ¨¡æµç¨‹
"""
import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.config import load_config, ensure_dir
from utils.logging import setup_logger
from utils.io import save_data, save_metadata

# Preprocessing
from preprocessing import AmazonPreprocessor, YelpPreprocessor

# Features
from features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
from features.discretize import create_discretization_summary

# Bayes
from bayes import BayesianNetworkStructure, CPDLearner, BayesianInference

# Evaluation
from evaluation import evaluate_model

logger = setup_logger("main")


class BayesReviewNetPipeline:
    """
    è´å¶æ–¯è¯„è®ºç½‘ç»œPipeline
    
    å®Œæ•´æµç¨‹ï¼š
    1. æ•°æ®é¢„å¤„ç†ï¼ˆPreprocessingï¼‰- æ”¯æŒAmazonå’ŒYelpæ•°æ®é›†
    2. ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰- Text + Behavior + Networkå¤šè§†è§’ç‰¹å¾
    3. è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ï¼ˆBayesian Networkï¼‰- DAGç»“æ„ä¸CPDå­¦ä¹ 
    4. æ¨æ–­ä¸è¯„ä¼°ï¼ˆInference & Evaluationï¼‰
    """
    
    # æ”¯æŒçš„æ•°æ®é›†
    SUPPORTED_DATASETS = ['amazon', 'yelp']
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        logger.info("="*80)
        logger.info("BayesReviewNet Pipeline åˆå§‹åŒ–")
        logger.info("="*80)
    
    def run(self, dataset_name: str, structure_type: str = 'default') -> dict:
        """
        è¿è¡Œå®Œæ•´Pipeline
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ('amazon', 'yelp')
            structure_type: è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡å­—å…¸
        """
        if dataset_name not in self.SUPPORTED_DATASETS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}ã€‚æ”¯æŒçš„æ•°æ®é›†: {self.SUPPORTED_DATASETS}")
        
        logger.info(f"\nå¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}\n")
        
        # ========== é˜¶æ®µ1: æ•°æ®é¢„å¤„ç† ==========
        logger.info("ã€é˜¶æ®µ1ã€‘æ•°æ®é¢„å¤„ç†")
        df = self._preprocess(dataset_name)
        
        # ========== é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹ ==========
        logger.info("\nã€é˜¶æ®µ2ã€‘ç‰¹å¾å·¥ç¨‹ - æå–Text + Behavior + Networkç‰¹å¾")
        df = self._extract_features(df)
        
        # ========== é˜¶æ®µ2.5: æ„é€ å¼±æ ‡ç­¾ ==========
        logger.info("\nã€é˜¶æ®µ2.5ã€‘å¼±æ ‡ç­¾æ„é€ ")
        df = self._construct_weak_labels(df, dataset_name)
        
        # ========== é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ ==========
        logger.info("\nã€é˜¶æ®µ3ã€‘è´å¶æ–¯ç½‘ç»œå»ºæ¨¡")
        structure, cpd_learner = self._build_bayesian_network(df, structure_type)
        
        # ========== é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼° ==========
        logger.info("\nã€é˜¶æ®µ4ã€‘æ¨æ–­ä¸è¯„ä¼°")
        df = self._inference_and_evaluate(df, structure, cpd_learner, dataset_name)
        
        # ========== ä¿å­˜æœ€ç»ˆç»“æœ ==========
        output_path = self._save_results(df, dataset_name)
        
        # ========== ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ ==========
        stats = self._generate_statistics(df, dataset_name, output_path)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ•°æ®é›† {dataset_name} å¤„ç†å®Œæˆï¼")
        logger.info(f"{'='*80}\n")
        
        return stats
    
    def _preprocess(self, dataset_name: str):
        """
        é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†
        
        æ‰€æœ‰æ•°æ®é›†ç»Ÿä¸€è¾“å‡ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„DataFrame:
        - user_id, item_id, review_id, timestamp, rating, review_text
        - platform, verified, vote
        - weak_label, label_source
        """
        if dataset_name == 'amazon':
            preprocessor = AmazonPreprocessor(
                self.config['data_paths']['amazon']['raw_dir']
            )
            sample_size = self.config['sampling']['amazon_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        elif dataset_name == 'yelp':
            preprocessor = YelpPreprocessor(
                self.config['data_paths']['yelp']['raw_dir']
            )
            sample_size = self.config['sampling']['yelp_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        save_data(df, f"{output_dir}/{dataset_name}_standardized.parquet")
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(df)} æ¡è®°å½•")
        return df
    
    def _extract_features(self, df):
        """
        é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹
        
        æå–å¤šè§†è§’ç‰¹å¾:
        - Textç‰¹å¾: æ–‡æœ¬ç»Ÿè®¡ã€æƒ…æ„Ÿã€ä¸»è§‚æ€§ç­‰
        - Behaviorç‰¹å¾: ç”¨æˆ·è¯„è®ºæ•°ã€è¯„åˆ†æ¨¡å¼ã€æ—¶é—´æ¨¡å¼ç­‰
        - Networkç‰¹å¾: ç”¨æˆ·-å•†å“å›¾ç»“æ„ç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        """
        # 2.1 æ–‡æœ¬ç‰¹å¾
        logger.info("  â†’ æå–Textç‰¹å¾...")
        text_extractor = TextFeatureExtractor()
        df = text_extractor.extract(df)
        
        # 2.2 è¡Œä¸ºç‰¹å¾
        logger.info("  â†’ æå–Behaviorç‰¹å¾...")
        behavior_extractor = BehaviorFeatureExtractor()
        df = behavior_extractor.extract(df)
        
        # TODO: 2.3 ç½‘ç»œç‰¹å¾ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        # logger.info("  â†’ æå–Networkç‰¹å¾...")
        # network_extractor = NetworkFeatureExtractor()
        # df = network_extractor.extract(df)
        
        # 2.4 ç‰¹å¾ç¦»æ•£åŒ–
        logger.info("  â†’ ç‰¹å¾ç¦»æ•£åŒ–...")
        discretizer = FeatureDiscretizer(self.config['discretization'])
        df = discretizer.discretize(df)
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len([c for c in df.columns if '_discrete' in c])} ä¸ªç¦»æ•£ç‰¹å¾")
        return df
    
    def _construct_weak_labels(self, df, dataset_name: str):
        """
        é˜¶æ®µ2.5: æ„é€ å¼±æ ‡ç­¾
        
        åŸºäºå¯å‘å¼è§„åˆ™æˆ–å¹³å°ä¿¡å·æ„é€ å¼±ç›‘ç£æ ‡ç­¾
        å¿…é¡»åœ¨ç‰¹å¾æå–ä¹‹åã€è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ä¹‹å‰æ‰§è¡Œ
        """
        from src.preprocessing.weak_labeling import construct_weak_label
        
        platform = df['platform'].iloc[0] if 'platform' in df.columns else dataset_name
        df = construct_weak_label(df, platform)
        
        return df
    
    def _build_bayesian_network(self, df, structure_type: str):
        """
        é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡
        
        åŸºäºå¤šè§†è§’ç‰¹å¾æ„å»ºè´å¶æ–¯ç½‘ç»œ
        """
        # 3.1 å®šä¹‰DAGç»“æ„
        structure = BayesianNetworkStructure()
        structure.define_structure(structure_type)
        
        logger.info(f"DAGç»“æ„å·²å®šä¹‰: {len(structure.edges)} æ¡è¾¹")
        logger.info(f"æ‹“æ‰‘æ’åº: {structure.get_topological_order()}")
        
        # 3.2 å­¦ä¹ CPD
        cpd_learner = CPDLearner(structure)
        cpd_learner.learn_cpds(df, smoothing=1.0)
        
        return structure, cpd_learner
    
    def _inference_and_evaluate(self, df, structure, cpd_learner, dataset_name: str):
        """
        é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼°
        
        ä½¿ç”¨è´å¶æ–¯æ¨æ–­è®¡ç®—åéªŒæ¦‚ç‡
        """
        # 4.1 è´å¶æ–¯æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        df = inference.infer_posterior(df, target_variable='weak_label')
        
        # 4.2 å¦‚æœæœ‰å¼±æ ‡ç­¾ï¼Œå¯ä»¥è¿›è¡Œè¯„ä¼°
        if 'weak_label' in df.columns and df['weak_label'].notna().any():
            logger.info("\nè¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            try:
                evaluation_result = evaluate_model(df)
                logger.info(f"  Precision: {evaluation_result['metrics']['precision']:.4f}")
                logger.info(f"  Recall: {evaluation_result['metrics']['recall']:.4f}")
                logger.info(f"  F1-Score: {evaluation_result['metrics']['f1']:.4f}")
                logger.info(f"  ROC-AUC: {evaluation_result['metrics'].get('roc_auc', 'N/A')}")
            except Exception as e:
                logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
        
        return df
    
    def _save_results(self, df, dataset_name: str) -> str:
        """
        ä¿å­˜æœ€ç»ˆç»“æœåˆ°data/processed/ç›®å½•
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        
        # ä¿å­˜æœ€ç»ˆå¤„ç†ç»“æœ
        output_path = f"{output_dir}/{dataset_name}_final.parquet"
        save_data(df, output_path)
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")
        return output_path
    
    def _generate_statistics(self, df, dataset_name: str, output_path: str) -> dict:
        """
        ç”Ÿæˆæ•°æ®é›†å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'dataset': dataset_name,
            'total_samples': len(df),
            'output_file': output_path,
            'features': {}
        }
        
        # ç»Ÿè®¡æ–‡æœ¬ç‰¹å¾
        text_features = [col for col in df.columns if col in [
            'review_length', 'sentiment_score', 'subjectivity_score',
            'exclamation_ratio', 'first_person_pronoun_ratio'
        ]]
        stats['features']['text'] = len(text_features)
        
        # ç»Ÿè®¡è¡Œä¸ºç‰¹å¾
        behavior_features = [col for col in df.columns if col.startswith('user_')]
        stats['features']['behavior'] = len(behavior_features)
        
        # ç»Ÿè®¡ç¦»æ•£åŒ–ç‰¹å¾
        discrete_features = [col for col in df.columns if col.endswith('_discrete')]
        stats['features']['discrete'] = len(discrete_features)
        
        # ç»Ÿè®¡å¼±æ ‡ç­¾åˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'weak_label' in df.columns:
            label_dist = df['weak_label'].value_counts().to_dict()
            stats['weak_label_distribution'] = {
                'suspicious': int(label_dist.get(1, 0)),
                'normal': int(label_dist.get(0, 0)),
                'missing': int(df['weak_label'].isna().sum())
            }
        
        # ç»Ÿè®¡åéªŒæ¦‚ç‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'weak_label_posterior_prob' in df.columns:
            posterior = df['weak_label_posterior_prob'].dropna()
            if len(posterior) > 0:
                stats['posterior_prob'] = {
                    'mean': float(posterior.mean()),
                    'median': float(posterior.median()),
                    'max': float(posterior.max()),
                    'samples_with_prob': len(posterior)
                }
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='BayesReviewNet - åŸºäºè´å¶æ–¯ç½‘ç»œçš„è™šå‡è¯„è®ºè¯†åˆ«ï¼ˆå¤šè§†è§’ç‰¹å¾ï¼‰'
    )
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--datasets',
        nargs='+',
        choices=['amazon', 'yelp', 'all'],
        default=['all'],
        help='è¦å¤„ç†çš„æ•°æ®é›† (amazon, yelp)'
    )
    parser.add_argument(
        '--structure',
        type=str,
        choices=['default', 'naive'],
        default='default',
        help='è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if 'all' in args.datasets:
        datasets = ['amazon', 'yelp']
    else:
        datasets = args.datasets
    
    # åˆå§‹åŒ–Pipeline
    pipeline = BayesReviewNetPipeline(args.config)
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    results = {}
    for dataset_name in datasets:
        try:
            stats = pipeline.run(dataset_name, args.structure)
            results[dataset_name] = stats
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}", exc_info=True)
            results[dataset_name] = {'status': 'failed', 'error': str(e)}
            continue
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    _print_summary(results)
    
    logger.info("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


def _print_summary(results: dict):
    """
    æ‰“å°å¤„ç†ç»“æœæ±‡æ€»
    
    Args:
        results: å„æ•°æ®é›†çš„å¤„ç†ç»“æœå­—å…¸
    """
    logger.info("\n" + "="*80)
    logger.info("å¤„ç†ç»“æœæ±‡æ€»")
    logger.info("="*80)
    
    for dataset_name, stats in results.items():
        if stats.get('status') == 'failed':
            logger.info(f"\nâŒ {dataset_name.upper()}: å¤„ç†å¤±è´¥")
            logger.info(f"   é”™è¯¯: {stats.get('error', 'Unknown')}")
            continue
        
        logger.info(f"\nâœ… {dataset_name.upper()}")
        logger.info(f"   æ ·æœ¬æ•°: {stats['total_samples']:,}")
        logger.info(f"   ç‰¹å¾ç»Ÿè®¡:")
        logger.info(f"      - Textç‰¹å¾: {stats['features']['text']} ä¸ª")
        logger.info(f"      - Behaviorç‰¹å¾: {stats['features']['behavior']} ä¸ª")
        logger.info(f"      - ç¦»æ•£åŒ–ç‰¹å¾: {stats['features']['discrete']} ä¸ª")
        
        # å¼±æ ‡ç­¾åˆ†å¸ƒ
        if 'weak_label_distribution' in stats:
            dist = stats['weak_label_distribution']
            total_labeled = dist['suspicious'] + dist['normal']
            if total_labeled > 0:
                susp_rate = dist['suspicious'] / total_labeled * 100
                logger.info(f"   å¼±æ ‡ç­¾åˆ†å¸ƒ:")
                logger.info(f"      - å¯ç–‘: {dist['suspicious']:,} ({susp_rate:.1f}%)")
                logger.info(f"      - æ­£å¸¸: {dist['normal']:,} ({100-susp_rate:.1f}%)")
                if dist['missing'] > 0:
                    logger.info(f"      - ç¼ºå¤±: {dist['missing']:,}")
        
        # åéªŒæ¦‚ç‡ç»Ÿè®¡
        if 'posterior_prob' in stats:
            post = stats['posterior_prob']
            logger.info(f"   åéªŒæ¦‚ç‡:")
            logger.info(f"      - å¹³å‡: {post['mean']:.4f}")
            logger.info(f"      - ä¸­ä½æ•°: {post['median']:.4f}")
            logger.info(f"      - æœ€å¤§å€¼: {post['max']:.4f}")
            logger.info(f"      - æœ‰æ•ˆæ ·æœ¬: {post['samples_with_prob']:,}")
        
        logger.info(f"   è¾“å‡ºæ–‡ä»¶: {stats['output_file']}")
    
    logger.info("\n" + "="*80)


if __name__ == '__main__':
    main()
