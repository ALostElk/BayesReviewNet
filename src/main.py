#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸»æ‰§è¡Œè„šæœ¬ - Pipelineè°ƒåº¦å™¨
åè°ƒæ•´ä¸ªè´å¶æ–¯ç½‘ç»œå»ºæ¨¡æµç¨‹
"""
import argparse
from pathlib import Path

from src.utils.config import load_config, ensure_dir
from src.utils.logging import setup_logger
from src.utils.io import save_data, save_metadata

# Preprocessing
from src.preprocessing import AmazonPreprocessor, YelpPreprocessor, OpSpamPreprocessor

# Features
from src.features import TextFeatureExtractor, BehaviorFeatureExtractor, FeatureDiscretizer
from src.features.discretize import create_discretization_summary

# Bayes
from src.bayes import BayesianNetworkStructure, CPDLearner, BayesianInference

# Evaluation
from src.evaluation import evaluate_model, OpSpamTestSet

logger = setup_logger("main")


class BayesReviewNetPipeline:
    """
    è´å¶æ–¯è¯„è®ºç½‘ç»œPipeline
    
    å®Œæ•´æµç¨‹ï¼š
    1. æ•°æ®é¢„å¤„ç†ï¼ˆPreprocessingï¼‰
    2. ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰
    3. è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ï¼ˆBayesian Networkï¼‰
    4. æ¨æ–­ä¸è¯„ä¼°ï¼ˆInference & Evaluationï¼‰
    """
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        logger.info("="*80)
        logger.info("BayesReviewNet Pipeline åˆå§‹åŒ–")
        logger.info("="*80)
    
    def run(self, dataset_name: str, structure_type: str = 'default'):
        """
        è¿è¡Œå®Œæ•´Pipeline
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ('amazon', 'opspam', 'yelp')
            structure_type: è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹
        """
        logger.info(f"\nå¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}\n")
        
        # ========== é˜¶æ®µ1: æ•°æ®é¢„å¤„ç† ==========
        logger.info("ã€é˜¶æ®µ1ã€‘æ•°æ®é¢„å¤„ç†")
        df = self._preprocess(dataset_name)
        
        # ========== é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹ ==========
        logger.info("\nã€é˜¶æ®µ2ã€‘ç‰¹å¾å·¥ç¨‹")
        df = self._extract_features(df)
        
        # ========== é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡ ==========
        logger.info("\nã€é˜¶æ®µ3ã€‘è´å¶æ–¯ç½‘ç»œå»ºæ¨¡")
        structure, cpd_learner = self._build_bayesian_network(df, structure_type)
        
        # ========== é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼° ==========
        logger.info("\nã€é˜¶æ®µ4ã€‘æ¨æ–­ä¸è¯„ä¼°")
        df = self._inference_and_evaluate(df, structure, cpd_learner, dataset_name)
        
        # ========== ä¿å­˜æœ€ç»ˆç»“æœ ==========
        self._save_results(df, dataset_name)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ•°æ®é›† {dataset_name} å¤„ç†å®Œæˆï¼")
        logger.info(f"{'='*80}\n")
    
    def _preprocess(self, dataset_name: str):
        """é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†"""
        if dataset_name == 'amazon':
            preprocessor = AmazonPreprocessor(
                self.config['data_paths']['amazon']['raw_dir']
            )
            sample_size = self.config['sampling']['amazon_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        elif dataset_name == 'opspam':
            preprocessor = OpSpamPreprocessor(
                self.config['data_paths']['opspam']['raw_dir']
            )
            df = preprocessor.load_and_standardize()
        
        elif dataset_name == 'yelp':
            preprocessor = YelpPreprocessor(
                self.config['data_paths']['yelp']['raw_dir']
            )
            sample_size = self.config['sampling']['yelp_sample_size'] \
                if self.config['sampling']['enabled'] else None
            df = preprocessor.load_and_standardize(sample_size)
        
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        
        # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
        output_dir = self.config['data_paths'][dataset_name]['processed_dir']
        ensure_dir(output_dir)
        save_data(df, f"{output_dir}/{dataset_name}_standardized.parquet")
        
        return df
    
    def _extract_features(self, df):
        """é˜¶æ®µ2: ç‰¹å¾å·¥ç¨‹"""
        # 2.1 æ–‡æœ¬ç‰¹å¾
        text_extractor = TextFeatureExtractor()
        df = text_extractor.extract(df)
        
        # 2.2 è¡Œä¸ºç‰¹å¾
        behavior_extractor = BehaviorFeatureExtractor()
        df = behavior_extractor.extract(df)
        
        # 2.3 ç‰¹å¾ç¦»æ•£åŒ–
        discretizer = FeatureDiscretizer(self.config['discretization'])
        df = discretizer.discretize(df)
        
        return df
    
    def _build_bayesian_network(self, df, structure_type: str):
        """é˜¶æ®µ3: è´å¶æ–¯ç½‘ç»œå»ºæ¨¡"""
        # 3.1 å®šä¹‰DAGç»“æ„
        structure = BayesianNetworkStructure()
        structure.define_structure(structure_type)
        
        logger.info(f"DAGç»“æ„å·²å®šä¹‰: {len(structure.edges)} æ¡è¾¹")
        logger.info(f"æ‹“æ‰‘æ’åº: {structure.get_topological_order()}")
        
        # 3.2 å­¦ä¹ CPD
        cpd_learner = CPDLearner(structure)
        cpd_learner.learn_cpds(df, smoothing=1.0)
        
        return structure, cpd_learner
    
    def _inference_and_evaluate(self, df, structure, cpd_learner, dataset_name: str):
        """é˜¶æ®µ4: æ¨æ–­ä¸è¯„ä¼°"""
        # 4.1 è´å¶æ–¯æ¨æ–­
        inference = BayesianInference(structure, cpd_learner)
        df = inference.infer_posterior(df, target_variable='weak_label')
        
        # 4.2 è¯„ä¼°ï¼ˆä»…OpSpamæœ‰ground truthï¼‰
        if dataset_name == 'opspam':
            evaluation_result = evaluate_model(df)
            logger.info(f"\nè¯„ä¼°ç»“æœ:")
            logger.info(f"  Precision: {evaluation_result['metrics']['precision']:.4f}")
            logger.info(f"  Recall: {evaluation_result['metrics']['recall']:.4f}")
            logger.info(f"  F1-Score: {evaluation_result['metrics']['f1']:.4f}")
            logger.info(f"  ROC-AUC: {evaluation_result['metrics'].get('roc_auc', 'N/A')}")
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            ensure_dir(self.config['output']['metadata_dir'])
            save_metadata(
                evaluation_result,
                f"{self.config['output']['metadata_dir']}/{dataset_name}_evaluation.yaml"
            )
        
        return df
    
    def _save_results(self, df, dataset_name: str):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        output_dir = self.config['output']['discretized_dir']
        ensure_dir(output_dir)
        
        # ä¿å­˜Parquetå’ŒCSV
        save_data(df, f"{output_dir}/{dataset_name}_final.parquet")
        save_data(df, f"{output_dir}/{dataset_name}_final.csv")
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° {output_dir}/")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='BayesReviewNet - åŸºäºè´å¶æ–¯ç½‘ç»œçš„è™šå‡è¯„è®ºè¯†åˆ«'
    )
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--datasets',
        nargs='+',
        choices=['amazon', 'opspam', 'yelp', 'all'],
        default=['all'],
        help='è¦å¤„ç†çš„æ•°æ®é›†'
    )
    parser.add_argument(
        '--structure',
        type=str,
        choices=['default', 'naive'],
        default='default',
        help='è´å¶æ–¯ç½‘ç»œç»“æ„ç±»å‹'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if 'all' in args.datasets:
        datasets = ['amazon', 'opspam', 'yelp']
    else:
        datasets = args.datasets
    
    # åˆå§‹åŒ–Pipeline
    pipeline = BayesReviewNetPipeline(args.config)
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset_name in datasets:
        try:
            pipeline.run(dataset_name, args.structure)
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}", exc_info=True)
            continue
    
    logger.info("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    main()

